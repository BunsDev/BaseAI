{"apiKey":"user_5yNqTvXdo5y4eiubtLsB22LfjCE81FxAs1Ro5nTjBnoREx38S9KcH9zh6dX4SYqrvqHE47MWNPx4TFmFYd8mq8o9","name":"local-llm","description":"A pipe that calls local llm using ollama","status":"public","model":"ollama:ben1t0/tiny-llm","stream":true,"json":false,"store":true,"moderate":true,"top_p":1,"max_tokens":1000,"temperature":0.7,"presence_penalty":1,"frequency_penalty":1,"stop":[],"tool_choice":"auto","parallel_tool_calls":false,"messages":[{"role":"system","content":"You are a helpful AI assistant. Less wordy."}],"variables":[],"memory":[],"tools":[]}
